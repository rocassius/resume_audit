## Power Forecast for W241 Project ##
# function for calculating the necessary number of job postings to respond to
calc_sample_size <- function(power, tau, p_control, alpha = 0.05) {
var.control <- p_control * (1 - p_control)
var.treat <-   (p_control + tau) * (1 - (p_control + tau))
z.alpha <- qnorm(1-alpha)
n <- (z.alpha - qnorm(1 - power)) ^ 2 * (var.control + var.treat) / (tau^2)
return(ceil(n))
}
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.15
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
## Power Forecast for W241 Project ##
# function for calculating the necessary number of job postings to respond to
calc_sample_size <- function(power, tau, p_control, alpha = 0.05) {
var.control <- p_control * (1 - p_control)
var.treat <-   (p_control + tau) * (1 - (p_control + tau))
z.alpha <- qnorm(1-alpha)
n <- (z.alpha - qnorm(1 - power)) ^ 2 * (var.control + var.treat) / (tau^2)
return(ceil(n))
}
?ceil
## Power Forecast for W241 Project ##
# function for calculating the necessary number of job postings to respond to
calc_sample_size <- function(power, tau, p_control, alpha = 0.05) {
var.control <- p_control * (1 - p_control)
var.treat <-   (p_control + tau) * (1 - (p_control + tau))
z.alpha <- qnorm(1-alpha)
n <- (z.alpha - qnorm(1 - power)) ^ 2 * (var.control + var.treat) / (tau^2)
return(ceiling(n))
}
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.15
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.15
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.15
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
taus <- seq(0.06, 0.20, 0.02)
sample_sizes <-
sapply(taus,
function(effect) calc_sample_size(power=0.8, tau = effect, p_control = 0.05))
# plot it
plot(sample_sizes ~ taus, xlim = c(0.06, 0.21), xlab = "tau", ylab = "postings",
main="Number of Postings vs Effect Size")
# works only in an R script
# with(text(sample_sizes ~ taus, labels = sample_sizes, pos = 4))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.15
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.10
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.10
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.20
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.15
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
58*2
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.10
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
# dafault settings (These are grounded in intuition and anecdotes)
pow <- 0.8
p_cont <- 0.05
tau <- 0.10
# Necessary number of postings with the defaults
n_job_postings <- calc_sample_size(power = pow, tau = tau, p_control = p_cont)
paste("Necessary Number of Job Postings: ", round(n_job_postings))
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(dplyr)
library(stargazer)
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
d[, dt := mids + qd]
names(d)
dim(d)
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_qd <- lm(qd ~ res + role + min_degree + days_open + company, data = d)
summary(cc_qd)
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
summary(cc_dt)
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(stargazer)
library(sandwich)
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
m1 <- lm(callback ~ mids + qd + mids*qd + res + company, data = d)
m1_cov_mat <- vcovHC(m1, type = 'HC1')
se1 <- sqrt(diag(m1_cov_mat))
m2 <- lm(callback ~ mids + qd + mids*qd + mids*role + role + res + company, data = d)
m2_cov_mat <- vcovHC(m2, type = 'HC1')
se2 <- sqrt(diag(m2_cov_mat))
m3 <- lm(callback ~ mids + qd + mids*qd + mids*role + role + days_open + DOA + res + company, data = d)
m3_cov_mat <- vcovHC(m3, type = 'HC1')
se3 <- sqrt(diag(m3_cov_mat))
se_list <- list(se1, se2, se3)
order = c('mids','qd','mids*qd','mids*role','role')
sink(file = "regression_table.tex")
cat("\\documentclass[11pt, oneside]{article} \n
\\usepackage{geometry}  \n
\\geometry{letterpaper} \n
\\usepackage{graphicx} \n
\\usepackage{amssymb} \n
\\begin{document} \n")
stargazer(m1,m2,m3,
type = 'latex',
covariate.labels = c("MIDS","MIDS x QD", "MIDS x Engineer", "MIDS x Scientist", "QD", "Engineer", "Scientist"),
dep.var.labels = "Callback",
se = se_list,
style = 'default',
omit = c('DOA','res','company'),
order = order)
cat("\\end{document}")
sink()
system("pdflatex regression_table.tex", ignore.stdout = F, wait = T, invisible = TRUE)
cc_qd <- lm(qd ~ res + role + min_degree + days_open + company, data = d)
summary(cc_qd)
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
d[, dt := mids + qd]
names(d)
dim(d)
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_qd <- lm(qd ~ res + role + min_degree + days_open + company, data = d)
summary(cc_qd)
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
summary(cc_dt)
stargazer(cc_mids, cc_qd, type='text', omit = 'company')
stargazer(cc_mids, cc_qd, type='latex', omit = 'company')
is.na(d)
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
d[, dt := mids + qd]
d[, min_degree := addNA(min_degree)]
names(d)
dim(d)
summary(is.na(d))
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_qd <- lm(qd ~ res + role + min_degree + days_open + company, data = d)
summary(cc_qd)
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
summary(cc_dt)
stargazer(cc_mids, cc_qd, type='latex', omit = 'company')
stargazer(cc_mids, cc_qd, type='latex', omit = 'company')
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
summary(cc_dt)
stargazer(cc_mids, cc_qd, type='text', omit = 'company')
library(data.table)
library(data.table)
library(dplyr)
library(lmtest)
library(sandwich)
library(ggplot2)
resumes <- c("Reilly Smith", "Sklyer Morris")
# randomizing functions
randomize.treat <- function(){ sample(0:1, 2) }
# load the data that was used
# df.mini <- read.csv("resume_data_outcomes_mini - resume.csv")
df.mini <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- read.csv("../data/resume_data_outcomes_mini.csv")
# load the data that was used
# df.mini <- read.csv("resume_data_outcomes_mini - resume.csv")
df.mini <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
d[, dt := mids + qd]
d[, min_degree := addNA(min_degree)]
names(d)
dim(d)
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(dplyr)
library(stargazer)
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
d[, dt := mids + qd]
d[, min_degree := addNA(min_degree)]
names(d)
dim(d)
summary(is.na(d))
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
sum(is.na(d))
sum(is.na(d))
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_qd <- lm(qd ~ res + role + min_degree + days_open + company, data = d)
summary(cc_qd)
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
summary(cc_dt)
cc_dt <- lm(dt ~ res + role + min_degree + days_open, data = d)
summary(cc_dt)
stargazer(cc_mids, cc_qd, type='text', omit = 'company')
m1
stargazer(m1, cc_qd, type='text', omit = 'company')
stargazer(m1, type='text', omit = 'company')
stargazer(m2, type='text', omit = 'company')
stargazer(m3, type='text', omit = 'company')
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]
d[, dt := mids + qd]
d[, min_degree := addNA(min_degree)]
names(d)
dim(d)
sum(is.na(d))
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_mids <- lm(mids ~ res + role + min_degree + days_open + company, data = d)
summary(cc_mids)
cc_qd <- lm(qd ~ res + role + min_degree + days_open + company, data = d)
summary(cc_qd)
res_qd <- lm(qd ~ res, data = d)
summary(res_qd)
