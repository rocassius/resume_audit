---
title: "Untitled"
author: "Rowan Cassius, Siliva Miramontes, Maurice Williams"
date: "4/18/2020"
output: pdf_document
header-includes:
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(stargazer)
library(sandwich) 
```



**Introduction**

Our study examines causal effects on interview callback rates for graduates from UC Berkeley’s online Master of Information and Data Science (MIDS) when compared to other Berkeley graduate degrees. We also measure the callback effect on the utilization of numerical versus non-numerical descriptors in resumes to depict the achievements and impacts on previous job experiences. To achieve this, we conduct a field experiment by responding to job postings for data focused positions with fictitious resumes.

For each job application, we deploy two fictitious resumes tailored to appear equally competitive. We then vary the graduate degree as MIDS or Statistics, along with the presence of numerical descriptors. This assignment allows us to compare and measure the difference in callback rates for these degree holders, numerical descriptors and their interaction. The results from this study can potentially inform MIDS administrators of how the program’s reputation has evolved since similar studies were conducted. Additionally, we can quantify whether failing to provide specific and measurable achievements on resumes results is a detriment of a job applicant. The researchers recognize that even though the experimental procedures were carefully deliberated, the impacts of the COVID-19 pandemic on the job market may have played an unprecedented role in our ability to gather valid outcomes.


**Motivation**

A quick web search for job announcements looking for data scientists reveals clear graduate degree preferences among employers. Listed among these degrees are the more familiar and well established classic disciplines of Mathematics, Physics, and Statistics, along with those we associate more with the 20th Century such as Electrical Engineering and Computer Science. Rarely do employers list a degree in Data Science among those they promote. One could hypothesize any number of possible motivations for this.

Recruiters and human resource departments often seek ways to reduce the search space among a pool of applicants. Employers seeking applicants for a data science role may be suspicious of the quality of these newly established programs. Several articles and discussion boards online contain professional data scientists from the more traditional quantitative disciplines expressing exactly this sort of concern. In their view, the lack of a track record to judge is paired with the assumption that the training provided by these programs to early cohorts suffer from their placement on the program administrators learning curve. This uncertainty would add an additional layer of risk to the already difficult task of identifying suitable candidates to fill data science vacancies. 

Current graduate students in data science may wonder if the preferences revealed in these online postings, and those described on job announcements, are indicative of disadvantages they may face in the data science labor market. Prospective students may even be steered toward more traditional quantitative tertiary education considered better preparation for the data science track. Administrators of these programs are likely aware of the potential impacts that questions of quality have on their admissions statistics and post graduate outcomes. It is reasonable to suspect that over time the reputations of specific programs will emerge more clearly. However, the current climate may still have an impact on current applicants to these positions.

For current and impending graduate students receiving their Master’s in Information and Data Science (MIDS) at the UC Berkeley School of Information, these concerns are of particular interest. While an expansive experiment that accounts for variation in outcomes for data science masters more broadly might reveal trends at a higher level, we focus on the potential outcomes for this specific group of would-be data scientists.

The second treatment we administered to some job applications of hypothetical candidates is a version of their professional impacts rewritten using numerical descriptions. Our experimental design tests whether the use of numerical descriptions in lieu of non numerical, yet equally informative descriptions causes measurably garners’ greater amounts of recruiters’ attention. 

The effect of the numerical descriptions factor is of interest for several reasons. It is both common wisdom of job hunters and the advice of many universities’ career services departments that candidates on the job market, especially recent university graduates, can make themselves more competitive by decorating their resumes with numbers which convey their impacts. For example, the “Job & Internship Guide” (2016-2017), provided by UC Berkeley’s Career Center, encourages job seekers with the following advice “Focus on the outcomes of your efforts...Quantify your results if possible”. Our experimental design seeks to provide some evidence either for or against this wisdom.

The numerical descriptions factor is also of interest because of explanations which researchers may promulgate a significant result favoring numerical descriptions were found. A significant result could be explained in numerous ways. It may be the case that recruiters have an easier time noticing where the most substantial impacts on a resume are located when the resume has distinguished those locations using numbers. Thus, recruiters who tiredly filter through tens or hundreds of resumes everyday can scan to numerically enhanced resumes’ moment of high impact with greater ease. Another explanation might be that numerically enhanced resumes garner more respect from their readers because they activate a deep-seated reverence of math and numbers which afflicts many in our society.

In full, our motivation for including both a data-science-graduate-degree factor and a numerical-descriptions factor together in a multifactor experiment does not stop with the motivations which drive each factor individually. Understanding how the inclusion of numerical descriptions impacts callback rates within the data science industry will also be of particular interest seeing as data science is, by nature, a uniquely number-reverent industry.

**Experimental Design**

Our interest in the dynamics previously described invites our first hypothesis: current resumes displaying a MIDS degree receive lower interest from employers relative to more traditional quantitative graduates of UC Berkeley. A second hypothesis would be as follows: the use of numerical descriptions in data science resumes provide higher callback rates from employers.

To answer these hypotheses, we design a set of fictitious resumes that are similar in every way and identify job announcements for data centric positions.  We then send a pair of applications to each available job position, we randomly vary the degree held and the use of numerical descriptors on the resume through blocked random assignment, with individual job announcements acting as blocks.

Recognizing the strong reputation of UC Berkeley’s academic programs, we sought to reduce any preference effects attached to the MIDS degree through its association with the broader campus. Additionally, we designed for a comparison between MIDS and a similarly scheduled graduate program. With these considerations in mind, we identified the Master of Arts in Statistics at UC Berkeley as the best alternate degree for comparison with MIDS. This full time program lasts roughly a year with similar credit requirements and a curriculum that includes mathematical statistics and statistical computing coursework. Additionally, both degrees are designed for industry work upon graduation, and both require a substantial data focused project as a requirement for graduation.


**Fictitious Candidates: Reilly Adrian Smith & Skyler T Morris**

Our approach calls for us to design a set of realistic yet fictitious resumes for graduate level employment opportunities in the data science industry. To do this, we engendered two fictitious candidates on the job market, and we took special care to ensure that the candidates were as close to equally competitive as possible in the way of their academic records and professional experiences. Additionally, both fictional candidates  were made out to have gone through a four year undergraduate program in Applied Math and Statistics at UC Berkeley and directly into a graduate level program at the same university.

When choosing the two full names to headline the fictitious resumes, we had them meet several criteria to simplify our experiment and increase the overall likelihood of callback. Each of the two names had to meet the following conditions:
* each name is a common Anglo name
* each name is gender neutral
* several pages of LinkedIn profiles have this name

Anglo in origin is a criterion because resume audits have already been conducted which measure the effects of race signals on application outcomes. Many of them unfortunately find significant negative effects disfavoring names of African American and Latin origin. As such, we choose anlgo names to boost the candidate’s potential outcomes. We required the names to be gender neutral so that the downstream experimental results are more likely to generalize to any gender. 

Finally, to validate that each name was sufficiently common, gender neutral and primarily used by people of caucasian descent, we searched each name on LinkedIn and checked the number of pages of people using the name along with the gender and race distributions associated with them. The first full names to meet these conditions were Reilly Smith and Skyler Morris.

**Fictitious Resume Content**

Each resume has a section titled “Experience” and a section titled “Projects”. It was in these sections that we fabricated and inserted fictional experiences comparable to those of a talented statistics or data science graduate student. Experiences were made up of industry internships, university research roles and TA positions for technical classes on campus. An example of experiences include the following:

![](../img/exp.png)

While an example of a item listed as under “Projects” is the following:

![](../img/proj.png)

To examine the exhaustive content in both experimental resumes, please see the appendix.

**Treatment Design**

We took special care in designing both the MIDS degree treatment and the numerical description treatment so that an exclusion restriction assumption about both of these factors would hold. For the degree treatment this design was straight forward. Consider the following two different degree signatures which we put on all resumes have the master’s degrees in statistics and master's degree in data science, respectively.

![](../img/stat.png)

![](../img/mids.png)


For the excludability assumption to hold any causal effect of the second signature relative to the first occurs only through the pathway of the degree “Information and Data Science” as opposed to the school of study, the “School of Information”. We are confident that this is a reasonable assumption because the College of Letters and Science and the School of Information at UC Berkeley are equally prestigious, and the university name, as opposed to the school is in bold	

On the other hand, it was challenging to design both numerical and non numerical descriptions of experiences which in all cases which both convey exactly the same information.  Consider the following table of examples:


|Non-Numerical Descriptor|Numerical Descriptor|
|------------------------|--------------------|
|The final model had production-level precision and recall and outperformed prior baseline models.|The final model had 88% precision and 87% recall and outperformed prior baseline models.|The final model had 88% precision and 87% recall and outperformed prior baseline models.|
|||
|Ideated an estimation method which substantially improved the reliability of estimators’ credible intervals.|Ideated an estimation method which substantially improved the reliability of estimators’ 95% credible intervals.|
|||
|Conducted a user study in which the great majority of subjects reported being completely satisfied with the product.|Conducted a user study in which 10 of 12 subjects reported being completely satisfied with the product.|
|||
|Bachelor of Arts, Applied Mathematics and Statistics, Double Major (High Distinction)|Bachelor of Arts, Applied Mathematics and Statistics, Double Major (GPA 3.87)|
|||
|Model harnesses a two-way data flow process, allowing for the update and improvement of network based on presented images.|Model harnesses a 2-way data flow process, allowing for the update and improvement of network based on presented images.|


For the excludability assumption about the numerical descriptions factor to hold, We must assume that a numerical description of item x contains the same substantive information as the non numerical description of item x. Let $I_n(x)$ denote the information contained in a numerical description of x while $I_e(x)$ denotes the information contained in an alternative english description. We did our best to have $I_n(x) = I_e(x)$ but note that it is okay if $I_e(x) > I_n(x)$ because our alternative hypothesis about this factor is one-side. Our strategies for maintaining $I_n(x) >= I_e(x)$ include the following: 


1. Using english words which convey a magnitude of impact at least as impressive as the impact conveyed with the numerically specified quantity. E.g, “great majority” as a counterpart to “10 of 12.” 
1. Relying on well known defaults, such as the 95% benchmark as a standard confidence level. In one item, we exploit this exact default by referring to an estimator’s “credible interval” in the non numerical description, and referring to it’s “95% credible interval”.
1. Using the english spelling of a quantity as opposed to conveying the quantity with a number: “two-way data flow process” vs “2-way data flow process”.

While we attempted to maintain and hereby assume that for any item x, I_n(x) >= I_e(x), the fact still remains that that a numerical description contains more specificity, and may therefore threaten the excludability assumption. If recruiters favor numerical descriptions more, they may do so because of an applicant’s ability to be specific (or simply recall their exact accomplishment). Although this threat to the excludability assumption lingers, according to our best judgement, it is small, so we proceed under the excludability assumption.

**Treatment Assignment**

In the experiment, we submitted 122 job applications for 61 different open data science positions at 61 different companies in the Bay Area. For each open position, we submit two applications the first of which is always by Reilly Smith and the second of which is by Skyler Morris. One of them is randomly assigned the master’s degree in statistics, while the other is assigned the data science degree. Similarly, one of them is randomly assigned their resume’s version of numerical descriptions, while the other applies their resume’s version with non-numerical descriptions. Thus, both treatments are blocked on the companies we apply to.

We plan to use linear regression to identify the causal effects of both the MIDS degree treatment and the numerical description treatment on interview callback rates. This requires that we assume non-interference between applications, which can conceivably not hold when two applications are sent to the same company. For example, recruiters might have a cutoff that ends at Reilly’s application, or Reilly Smith’s application could conceivably impress a recruiter so much that the recruiter then views Skyer Morris’s application less favorably than they would have if they had seen Skyler’s first. We are comfortable proceeding with the non-interference assumption nonetheless because the resumes were designed to be equally competitive across a broad range of jobs. In addition, data science roles in the Bay area receive tremendous volumes of applicants (usually in the hundreds), and we doubt that seasoned recruiters would be influenced so greatly in the flow of their resume sifting.

Lastly, the blocked random assignment of treatments for a given application was done using the pseudo-random number generation facility in R and was done so with a seed so that it may be reproduced. The random assignment was prespecified using up to 1000 applications before the deployment of applications. 


**Determining Sample Size**

According to a power calculation, we decided to send at least 120 applications (and eventually sent 122). To make this calculation, we had to speculate about the base application callback rate and the true effect size. These speculations are based on the personal experience of one of the researchers when talking to a data scientist at Esurance. When offering advice about how to get more attention from recruiters, he claimed that adding numbers to his resume made a significant difference in his job hunt, raising his callback rate from 5% to 25%. Respecting the possibility that he was exaggerating we proceeded to estimate that resumes with numerical descriptions have a 5% callback rate, while resumes enhanced with numerical descriptions have a 20% callback rate. In order to have 80% power, we then calculated the necessary sample size under these estimates:

<the derivation>

**Application Deployment**
 
To identify potential job announcements for our experimental blocks, we used the popular job search engines Google Jobs and Indeed. While Indeed acts as its own job posting site, Google Jobs aggregates job announcements from other job boards. This resulted in applications going through a multitude of portals, but the lion’s share went through the Indeed and Glassdoor sites.

Suitable job announcements were identified within a set of criteria. For location, we searched for open positions advertised in the following nine counties:

||||
|-|-|-|
|Alameda|Napa|Santa Clara|
|Contra Costa|San Francisco|Solano|
|Marin|San Mateo|Sonoma|

These counties all fall within a reasonable distance from the University of California, Berkeley campus. As such, they are more likely to be familiar with the university and the graduates they produce. It should be noted that while the MIDS program is fashioned as a remote program capable of delivering their education to an international student body, our desire to compare MIDS to other Berkeley STEM graduates pushes us to consider proximity. Additionally, many of these counties have a high density of firms looking to hire data professionals relative to counties in other parts of California, as well as other states.

Additionally, we sought job announcements with similar credentialing and professional experiences requirements. With our fictional candidates having gone straight through undergraduate to a graduate program, we collected applications to job listings that sought individuals at the entry level, or with less than four years of experience.

During the submission process, we encountered some instances in which employers required applicants to submit cover letters and LinkedIn profiles. To keep the anonymity of our experiment and maximize the number of applications submitted, we formulated generic cover letters, and a statement explaining our displeasure with LinkedIn’s user data practices. The templates are displayed in the Appendix


**Data Collection**

Our analysis required us to collect employer callbacks for each of the 4 treatment groups with content from 2 resumes. This gave us 8 resumes in which to monitor and collect indications of employer interest. To streamline this process, we generated 8 GMail accounts in which to collect any responses from interested employers. Additionally, we used 4 Google Voice phone numbers attached to each of the four treatment groups, with the intention of scanning voicemails left for our candidates and distinguishing between application responses using the fake resume names mentioned.

The recording of employer responses came primarily from emails expressing interest in our fictional job candidates, with a handful coming into our Google Voice inboxes. For each email response received, we use the content of the message left by the employer to match the response to the corresponding resume. This process was made highly efficient by allocating ech resume variant its own designated email, allowing for a simple identification of firm and position applied to for recording callbacks.

When considering the types of responses our fictional applicants might receive, we decided to collect two types of callbacks. First, any response expressing interest would be considered a positive callback, with responses of disinterest, or no contact at all, resulting in a non-positive response. The second type of response we collected is specific to technical workers: a request to complete a small project as part of the application process.


**Attrition**

Requests for completing a project to move forward with the interview process presents challenges to our identification strategy. It is not well established if requests of this sort by employers are an indication of interest in a candidate, or if projects are used to thin the pool of qualified applicants by being distributed to all who’ve applied. This is of special concern if the projects are designed for automated evaluation. With these concerns in mind, we decided to collect an additional outcome variable for a positive callback being a project request.

Our study showed signs that requests for projects were automatically generated for each applicant, circumventing the evaluation process needed to determine the outcome of the subject has they been evaluated and the treatment conditions exposed. We therefore conclude that requests for projects should be treated as attrition.

Given the evidence that projects would be automatically generated for the pair of resumes sent to an employer, we can assume that propensity to attrite is independent of treatment assignment. However, systematic differences between firms that request projects and those that don’t may be correlated with potential outcomes. For instance, technology firms may be more likely to request projects relative to those in the healthcare sector. It may also be that tech firms tend to prefer more traditional graduate programs over new ones such as MIDS. We then cannot say that attrition is independent of potential outcomes.


**Pre-Treatment Covariates and Heterogeneous Treatment Effects**

We considered possible heterogeneous treatment effects on some characteristics of job announcements. These covariates were collected during the application deployment process. 

**_Covariates of interest_**:

*Minimum Degree Required* (M)

The minimum degree required per job application varied between Bachelor’s and Master’s degrees. This requirement may reveal the employer’s preference, as those that require a graduate degree at a minimum, may be more inclined to hire statisticians, as opposed to individuals holding a Master’s degree in data science with less mathematical exposure.

*Type of Role* (analyst or scientist) (R)

Consider the interaction coefficient between the type of role and the type of degree assigned per submitted application. An applicant with a Master’s in Statistics could potentially be a better candidate for the position of a “scientist”, as opposed to an applicant with a MIDS degree, who from the perspective of a subject, may only be considered a competitive candidate for a position as an analyst.

*Number of Days Posted* (DP) & *Date of Submission* (DS)

Lastly, we considered the days passed since the job listing was posted and the day of application submission as measurements of time. The amount of time (in days) it took for the researcher to submit applications to job announcements that had been posted days prior to our submission, may put some preference toward applications that were submitted earlier. This may place our fictitious applicants at a disadvantage, since company recruiters may have already selected potential candidates prior to the exposure of our fictitious applicant. Additionally, the amount of days since the job posting relative to the date of submission may also affect the callback rates as the position might have already been filled.

We represent the set of covariates in vector form $C_i$:

$$C_i^T = (M, R, DP, DS) $$

**Covariate Balance Check**

As a check for covariate balance between experimental treatments, we regress MIDS and ND on $C_i$. While most of the covariates are balanced between experimental groups, the indicator Skyler Morris’s resume content, however, is highly significantly correlated with the MIDS treatment indicator. 

In response, we take several steps to ensure that the significant correlation between MIDS and the Skyler Morris indicator is a spurious correlation and not a result of a flawed randomization process. We first reproduce the randomization with the same R code and random seed that was initially used to produce the randomization. Next we create 1000 more possible randomizations using the same randomization code, and collect the estimated effects of the Skyler Morris Indicator on the MIDS treatment. A descriptive summary of the Skyler Morris effects on the MIDS treatment is below:

\<table to go here\>

The percentile of original Sklyer Morris effect estimate of  2.917 is 98.2% relative to the empirically estimated distribution of the effect estimate. This percentile is consistent with the p-value of 0.9% that is associated with the original estimate. The distribution of the Skyler Morris effect on MIDS under the same randomization procedure that was originally used is approximately normally distributed about 0. This provides evidence that the correlation between Skyer Morris and MIDS that was present in the experiment’s treatment assignment is indeed spurious, and there is otherwise covariate balance across the experimental groups.

**Identification Strategy**

Our experiment employs a factorial 2x2 design with two factors: degree and descriptor. A straightforward identification strategy is to implement a difference estimator for each factor. We thus estimate a models of the following linear form:

$$Y_{ik} = a + M_i\beta_1 + Q_i\beta_2 + M_i*Q_i*\beta_3 + M_iS_i\beta_4 + M_iE_i\beta_5 + \gamma  C_i u_i$$

where $M_i$ and $Q_i$ are the MIDS and numerical-descriptor treatment indicators for application i sent to job announcement $k$, $S_i$ and $E_i$ are indicators for the job role designating scientist and engineer respectively and with analyst as the omitted group. The vector $C_i$ contains all pre-treatment covariates and $u_i$ is the idiosyncratic error.

**Responses**

Out of 122 applications sent out, we received 8 total responses, 4 of which were requests for projects. Below is a table showing the responses gathered, dropping those who attrite, concluding the trial period of March 11th - April 9th. 

||Numerical|None-Numerica|
|:-|:-:|:-:|
|MIDS|1|1|
|Stats|1|1

As we can see, the four responses are distributed in such a way as to provide our study with zero information to challenge our null hypotheses. As a result of this distribution and the high proportion of attrition, any regression table would fail to provide any additional information to our writeup (though one has been provided in the Appendix).

**A Note on COVID-19**

Our trial period began March 10th of 2020. Seven days later on Match 17th, the nine counties in the Bay Area issued a shelter at home order in response to the COVID-19 pandemic taking hold in the United States. We received some feedback that this exogenous factor greatly impaired our ability to collect valid observations for our subjects. We received the following correspondence from one positive callback:


“Hi Skyler,
Thank you for your time spent with us for the Data Scientist role. At this time, we have decided to place this position on hold, and therefore need to cancel our scheduled phone interview. When the role re-opens, we will be sure to contact you.
We apologize for any inconvenience this has caused.”
This response demonstrates how the shelter in place order impacted hiring and employment across the region. Later in the month, new unemployment claims soared as millions of Californians lost their jobs due to business uncertainty surrounding the pandemic. While many of these jobs come from the service sector, there is anecdotal evidence that job losses in firms hiring data scientists were also shedding workers, suggesting that these sectors may have  been undergoing a hiring freeze at the least. In order to gain confidence in any conclusions drawn from our experimental design, a followup study should be conducted.

**Conclusion**

In this study, we attempt to examine the effects of holding a Masters in Information and Data Science when compared to more traditional graduate programs, here as a Masters in Statistics, as well as the detrimental impact of not including numerical descriptors to convey the impacts of previous work experience. We designed a set of fictitious resumes of comparable quality in academic and work experience, varying the degree type and descriptors used. We sent out 122 application pairs to 61 job announcements to measure the callback rate differences between our treatment groups.









\vspace{30pt}

**APPENDIX**

Below we display a table of the fictitious applicants with their corresponding email addresses, phone numbers along with their treatment group designation.


|Treatment Group|Stats,ED|MIDS,ED|Stats,QD|MIDS,QD|
|-|-|-|-|-|
|Treat Tag|Name Resume|Name, Resume|Name - Resume|Name | Resume|
|Phone| \*\*\*-\*\*\*-4787|\*\*\*-\*\*\*-6356|\*\*\*-\*\*\*-2827|\*\*\*-\*\*\*-0539|
Skyler|skyler.\*\*.morris\@\*|skyler.\*\*.morris\@\*|skyler.\*\*.morris\@\*|skyler.\*\*.morris\@\*|
|Reilly|reilly\*\*smith\@\*|reilly\*\*smith\@\*|reilly\*\*smith\@\*m|reillyd\*\*smith\@\*|


**_LinkedIn responses_**:

Reilly’s:

*I refuse to use linkedin out of data privacy concerns, and I am happy to discuss why in later stages. Thank you for understanding.*

Skyler’s:

*Linkedin has data handling practices which concern me deeply because they compromise user privacy. If you would like to discuss this further, please don’t hesitate to reach out to me.*

**_Cover letters_**:

Reilly’s:

*Hi, my name is Reilly Smith, and I have acquired a broad range of data science experience through my coursework at UC Berkeley, academic research and industry internships. I believe that I am a great fit for <company name> because I like its unique mission and I have the qualifications necessary to succeed on a fast-paced data science team. I look forward to the opportunity to work with you.*

Skyler’s:

*I enjoy all branches of data science from machine learning to analytics to data engineering. Combined, my data science qualifications, interest in the <industry name> industry and desire to work at a forward thinking company make me a great fit for a role as a data scientist at <company name>. *


<!-- Regression Table -->

```{r echo = F, include = F}
d <- read.csv("../data/resume_data_outcomes_mini.csv")
d <- data.table(d)#[,.(callback,project,res,mids,qd,role,min_degree,DOA,days_open)]


m1 <- lm(callback ~ mids + qd + mids*qd + res + company, data = d)
m1_cov_mat <- vcovHC(m1, type = 'HC1')
se1 <- sqrt(diag(m1_cov_mat))

m2 <- lm(callback ~ mids + qd + mids*qd + mids*role + role + res + company, data = d)
m2_cov_mat <- vcovHC(m2, type = 'HC1')
se2 <- sqrt(diag(m2_cov_mat))

m3 <- lm(callback ~ mids + qd + mids*qd + mids*role + role + days_open + DOA + res + company, data = d)
m3_cov_mat <- vcovHC(m3, type = 'HC1')
se3 <- sqrt(diag(m3_cov_mat))


se_list <- list(se1, se2, se3)

order = c('mids','qd','mids*qd','mids*role','role')


sink(file = "regression_table.tex")

cat("\\documentclass[11pt, oneside]{article} \n
    \\usepackage{geometry}  \n
    \\geometry{letterpaper} \n
    \\usepackage{graphicx} \n
    \\usepackage{amssymb} \n
    \\begin{document} \n",
stargazer(m1,m2,m3,
          type = 'latex',
          covariate.labels = c("MIDS","MIDS x QD", "MIDS X Engineer", "MIDS x Scientist", "QD", "Engineer", "Scientist"),
          dep.var.labels = "Callback",
          se = se_list,
          style = 'default',
          omit = c('DOA','res','company'),
          order = order),
"\\end{document}")

sink()
system("pdflatex regression_table.tex", ignore.stdout = T)
```

![](./regression_table.pdf)

